{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57123cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: better beat tracking through robust onset aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86bae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HUGE refactor on this one. lesgo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27068dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mir_eval\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeae2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1bc1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset available in https://www.eumus.edu.uy/candombe/datasets/ISMIR2015/dataset.html\n",
    "file_path = [i[:-4] for i in glob.glob('../../datasets/candombe/*.wav')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1d8b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiments settings\n",
    "FS = 22050\n",
    "N_FFT = 2048\n",
    "MEL_BANDS = 128\n",
    "CUTOFF = 8000\n",
    "HOP_SIZE = 64\n",
    "\n",
    "DATASET_PATH = '../../datasets/candombe/'\n",
    "\n",
    "OUTPUT_CSV_PATH = 'experiments_results/robust_onset_aggregation'\n",
    "BASELINE_DEFAULT_CSV_PATH = os.path.join(OUTPUT_CSV_PATH, 'baseline_default.csv')\n",
    "BASELINE_PARAMETERS_CSV_PATH = os.path.join(OUTPUT_CSV_PATH, 'baseline_parameters.csv')\n",
    "MEDIAN_CSV_PATH = os.path.join(OUTPUT_CSV_PATH, 'median.csv')\n",
    "MEAN_CSV_PATH = os.path.join(OUTPUT_CSV_PATH, 'mean.csv')\n",
    "BASELINE_DEFAULT_PATH = os.path.join(OUTPUT_CSV_PATH, 'baseline_default')\n",
    "BASELINE_PARAMETERS_PATH = os.path.join(OUTPUT_CSV_PATH, 'baseline_parameters')\n",
    "MEDIAN_PATH = os.path.join(OUTPUT_CSV_PATH, 'median')\n",
    "MEAN_PATH = os.path.join(OUTPUT_CSV_PATH, 'mean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b61c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in file_path:\n",
    "    print(os.path.join(BASELINE_DEFAULT_PATH, os.path.basename(file)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b00f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(dataset_folder, output_folder, onset_parameters, beat_parameters, override):\n",
    "    for file in dataset_folder:\n",
    "        file_npz = os.path.join(output_folder, os.path.basename(file)) + '.npz'\n",
    "        \n",
    "        if not os.path.isfile(file_npz) or override:\n",
    "            print(f\"processing {file}\")\n",
    "            x, fs = librosa.load(f\"{file}.wav\", mono=True, sr=FS)\n",
    "            x_df = pd.read_csv(f\"{file}.csv\", names=[\"timestamp\", \"beat\"])\n",
    "            ground_truth = x_df[\"timestamp\"].values\n",
    "\n",
    "            onset_parameters[\"y\"] = x\n",
    "            onset_parameters[\"sr\"] = FS\n",
    "\n",
    "            # the standard method from librosa already uses 128 mel bands by defaul\n",
    "            # so we can just skip this (:\n",
    "            onset_subbands = librosa.onset.onset_strength_multi(**onset_parameters)\n",
    "            \n",
    "            beat_parameters[\"onset_envelope\"] = onset_subbands[0]\n",
    "            beat_parameters[\"sr\"] = FS\n",
    "            \n",
    "            bpm, beat_frame = librosa.beat.beat_track(**beat_parameters)\n",
    "            beat_timestamps = librosa.frames_to_time(beat_frame, FS)\n",
    "\n",
    "            print(f\"saving {file}.npz\")\n",
    "            np.savez(\n",
    "                file_npz, \n",
    "                onset=onset_subbands[0], \n",
    "                reference=ground_truth,\n",
    "                estimated=beat_timestamps\n",
    "            )\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acdf865",
   "metadata": {},
   "source": [
    "## test refactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66f6809",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "baseline_onset_configs = {}\n",
    "baseline_beat_configs = {}\n",
    "run(file_path, BASELINE_DEFAULT_PATH, baseline_onset_configs, baseline_beat_configs, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442a0583",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_DEFAULT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52f728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, _, file in os.walk(BASELINE_DEFAULT_PATH):\n",
    "    baseline_files = file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20c80ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_files = [os.path.join(BASELINE_DEFAULT_PATH, i) for i in baseline_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020871c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.load(baseline_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2788fe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp['onset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e693f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fe3fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(np.savez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4103fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.walk(BASELINE_DEFAULT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491da412",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_baseline_parameters = {}\n",
    "if glob.glob(BASELINE_PARAMETERS_CSV_PATH) == []:\n",
    "    \n",
    "    for file in file_path:\n",
    "        print(f\"processing {file}\")\n",
    "        x, fs = librosa.load(f\"{file}.wav\", mono=True, sr=FS)\n",
    "        x_df = pd.read_csv(f\"{file}.csv\", names=[\"timestamp\", \"beat\"])\n",
    "        ground_truth = x_df['timestamp'].values\n",
    "\n",
    "        # calculate the beats using librosa approach\n",
    "        onset_subbands = librosa.onset.onset_strength_multi(\n",
    "            y=x, \n",
    "            sr=FS, \n",
    "            n_fft = N_FFT,\n",
    "            hop_length = HOP_SIZE,\n",
    "        )\n",
    "        bpm, beat_frame = librosa.beat.beat_track(onset_envelope=onset_subbands[0], sr=FS)\n",
    "        beat_timestamps = librosa.frames_to_time(beat_frame, FS)\n",
    "        \n",
    "        print(f\"saving {file}\")\n",
    "        np.savez(\n",
    "            os.path.join(BASELINE_PARAMETERS_PATH, os.path.basename(file)), \n",
    "            onset=onset_subbands[0], \n",
    "            reference=ground_truth,\n",
    "            estimated=beat_timestamps\n",
    "        )\n",
    "\n",
    "        #dataset_median[file]= mir_eval.beat.evaluate(ground_truth, librosa_timestamps)\n",
    "    #pd.DataFrame(dataset_baseline_parameters).to_csv(BASELINE_PARAMETERS_PATH, index=False)\n",
    "else:\n",
    "    print(\"loading file\")\n",
    "    candombe_csv = pd.read_csv(BASELINE_PARAMETERS_PATH, index_col=0)\n",
    "    dataset_baseline_parameters = candombe_csv.to_dict()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b05195",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_median = {}\n",
    "\n",
    "if glob.glob(MEDIAN_CSV_PATH) == []:\n",
    "    for file in file_path:\n",
    "        print(f\"processing {file}\")\n",
    "        x, fs = librosa.load(f\"{file}.wav\", mono=True, sr=FS)\n",
    "        x_df = pd.read_csv(f\"{file}.csv\", names=[\"timestamp\", \"beat\"])\n",
    "        ground_truth = x_df['timestamp'].values\n",
    "\n",
    "        # the standard method from librosa already uses 128 mel bands by defaul\n",
    "        # so we can just skip this (:\n",
    "        onset_subbands = librosa.onset.onset_strength_multi(\n",
    "            y=x, \n",
    "            sr=FS, \n",
    "            n_fft = N_FFT,\n",
    "            hop_length = HOP_SIZE,\n",
    "            aggregate = np.median\n",
    "        )\n",
    "        bpm, beat_frame = librosa.beat.beat_track(onset_envelope=onset_subbands[0], sr=FS)\n",
    "        beat_timestamps = librosa.frames_to_time(beat_frame, FS)\n",
    "        \n",
    "        np.savez(\n",
    "            os.path.join(MEDIAN_PATH, os.path.basename(file)), \n",
    "            onset=onset_subbands[0], \n",
    "            reference=ground_truth,\n",
    "            estimated=beat_timestamps\n",
    "        )\n",
    "\n",
    "        #dataset_median[file]= mir_eval.beat.evaluate(ground_truth, librosa_timestamps)\n",
    "        \n",
    "    #pd.DataFrame(dataset_median).to_csv(MEDIAN_PATH, index=False)\n",
    "else:\n",
    "    print(\"loading file\")\n",
    "    candombe_csv = pd.read_csv(MEDIAN_PATH, index_col=0)\n",
    "    dataset_median = candombe_csv.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c430a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mean = {}\n",
    "\n",
    "if glob.glob(MEAN_CSV_PATH) == []:\n",
    "    for file in file_path:\n",
    "        print(f\"processing {file}\")\n",
    "        x, fs = librosa.load(f\"{file}.wav\", mono=True, sr=FS)\n",
    "        x_df = pd.read_csv(f\"{file}.csv\", names=[\"timestamp\", \"beat\"])\n",
    "        ground_truth = x_df['timestamp'].values\n",
    "\n",
    "        # the standard method from librosa already uses 128 mel bands by defaul\n",
    "        # so we can just skip this (:\n",
    "        \n",
    "        if not os.path.isfile(os.path.join(MEAN_PATH, os.path.basename(file))):\n",
    "            onset_subbands = librosa.onset.onset_strength_multi(\n",
    "                y=x, \n",
    "                sr=FS, \n",
    "                n_fft = N_FFT,\n",
    "                hop_length = HOP_SIZE,\n",
    "                aggregate = np.mean\n",
    "            )\n",
    "            bpm, beat_frame = librosa.beat.beat_track(onset_envelope=onset_subbands[0], sr=FS)\n",
    "            beat_timestamps = librosa.frames_to_time(beat_frame, FS)\n",
    "\n",
    "            np.savez(\n",
    "                os.path.join(MEAN_PATH, os.path.basename(file)), \n",
    "                onset=onset_subbands[0], \n",
    "                reference=ground_truth,\n",
    "                estimated=beat_timestamps\n",
    "            )\n",
    "\n",
    "        #dataset_median[file]= mir_eval.beat.evaluate(ground_truth, librosa_timestamps)\n",
    "        \n",
    "    #pd.DataFrame(dataset_median).to_csv(MEDIAN_PATH, index=False)\n",
    "else:\n",
    "    print(\"loading file\")\n",
    "    candombe_csv = pd.read_csv(MEDIAN_PATH, index_col=0)\n",
    "    dataset_median = candombe_csv.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18efde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(SUM_PATH).transpose().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63acd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_baseline_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009df863",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dataset_baseline_default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cfaf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline_default = pd.DataFrame(dataset_baseline_default).transpose().reset_index()\n",
    "df_baseline_parameters = pd.DataFrame(dataset_baseline_parameters).transpose().reset_index()\n",
    "df_median = pd.DataFrame(dataset_median).transpose().reset_index()\n",
    "df_sum = pd.DataFrame(dataset_sum).transpose().reset_index()\n",
    "df_max = pd.DataFrame(dataset_sum).transpose().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f99d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline_default.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9869a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_median.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73083703",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sum.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142c9530",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_max.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78acaec",
   "metadata": {},
   "source": [
    "No caso desse dataset, o max, median e sum tiveram resultados bastante similares. Isso pode ser porque eles pegam os componentes mais fortes de onsets e na hora de agregar são \"puxados\" para os mesmos resultados? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779ff970",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_median = df_median.loc[0]\n",
    "example_baseline = df_baseline.loc[0]\n",
    "example_sum = df_sum.loc[0]\n",
    "example_max = df_max.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8c1a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, fs = librosa.load(f\"{example_baseline['index']}.wav\", mono=True, sr=FS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a3895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 25\n",
    "end = 30\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, sharex=True)\n",
    "ax.plot(x[start*fs:end*fs], alpha=0.5)\n",
    "\n",
    "ax.vlines(\n",
    "    (example_baseline.ground_truth_beats[\n",
    "        (example_baseline.ground_truth_beats >= start) & (example_baseline.ground_truth_beats <= end)]-start)*FS, \n",
    "          0, 0.6, alpha=0.9, color='r', linestyle='-', label='groundtruth'\n",
    ")\n",
    "\n",
    "ax.vlines(\n",
    "    (example_baseline.librosa_beats[\n",
    "        (example_baseline.librosa_beats >= start) & (example_baseline.librosa_beats <= end)]-start)*FS, \n",
    "    0, 0.6, alpha=0.5, color='g', linestyle='--', label='baseline')\n",
    "\n",
    "\n",
    "ax.vlines(\n",
    "    (example_median.librosa_beats[\n",
    "        (example_median.librosa_beats >= start) & (example_median.librosa_beats <= end)]-start)*FS, \n",
    "    0, 0.6, alpha=0.5, color='b', linestyle='--', label='median')\n",
    "\n",
    "ax.vlines(\n",
    "    (example_sum.librosa_beats[\n",
    "        (example_sum.librosa_beats >= start) & (example_sum.librosa_beats <= end)]-start)*FS, \n",
    "    0, 0.6, alpha=0.5, color='b', linestyle='-', label='sum')\n",
    "\n",
    "ax.vlines(\n",
    "    (example_max.librosa_beats[\n",
    "        (example_max.librosa_beats >= start) & (example_max.librosa_beats <= end)]-start)*FS, \n",
    "    0, 0.6, alpha=0.5, color='r', linestyle='--', label='max')\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820ad9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# o que eu quero plotar?\n",
    "# 1. um trecho da música original\n",
    "# 2. os beats desse trecho\n",
    "# 3. os beats detectados do mcfee\n",
    "# 4. os beats detectados do librosa sem usar a mediana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1116bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# talvez faça sentido também olhar só pra função de onset pra entender\n",
    "# por que os resultados estão TÃO horrorosos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e030f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
